# -*- coding: utf-8 -*-
"""MCP CKD Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lDzuIiwoHhAusV-X_Ww4s9wtJHjq0P5l
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

df= pd.read_csv('/content/ckd.csv')
df.head()

df.shape

df.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',
              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',
              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',
              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',
              'aanemia', 'class']

df.describe()

df.info()

df = df.replace('?', float('nan'))
df

df.info()

df['age'] = pd.to_numeric(df['age'], errors='coerce')
df['blood_pressure'] = pd.to_numeric(df['blood_pressure'], errors='coerce')
df['specific_gravity'] = pd.to_numeric(df['specific_gravity'], errors='coerce')
df['albumin'] = pd.to_numeric(df['albumin'], errors='coerce')
df['sugar'] = pd.to_numeric(df['sugar'], errors='coerce')
df['blood_glucose_random'] = pd.to_numeric(df['blood_glucose_random'], errors='coerce')
df['blood_urea'] = pd.to_numeric(df['blood_urea'], errors='coerce')
df['serum_creatinine'] = pd.to_numeric(df['serum_creatinine'], errors='coerce')
df['sodium'] = pd.to_numeric(df['sodium'], errors='coerce')
df['potassium'] = pd.to_numeric(df['potassium'], errors='coerce')
df['haemoglobin'] = pd.to_numeric(df['haemoglobin'], errors='coerce')
df['packed_cell_volume'] = pd.to_numeric(df['packed_cell_volume'], errors='coerce')
df['white_blood_cell_count'] = pd.to_numeric(df['white_blood_cell_count'], errors='coerce')
df['red_blood_cell_count'] = pd.to_numeric(df['red_blood_cell_count'], errors='coerce')

# Extracting categorical and numerical columns

cat_cols = [col for col in df.columns if df[col].dtype == 'object']
num_cols = [col for col in df.columns if df[col].dtype != 'object']

for col in cat_cols:
    unique_values = df[col].unique()
    print(f'Unique values in {col}: {unique_values}')

df['class'] = df['class'].map({'ckd': 0, 'notckd': 1})
df['class'] = pd.to_numeric(df['class'], errors='coerce')

plt.figure(figsize = (10,6))

sns.heatmap(df.corr(), annot = True, linewidths = 2, linecolor = 'lightgrey')
plt.show()

df

df.isna().sum().sort_values(ascending = False)

df[num_cols].isnull().sum()

df[cat_cols].isnull().sum()

df

for column in num_cols:
    df[column].fillna(df[column].mean(), inplace=True)
#for column in cat_cols:
    #df[column].fillna('Unknown', inplace=True)
    #mode_value = df[col].mode()[0]
    #df[col].fillna(mode_value, inplace=True)
for column in cat_cols:
    mode_value = df[column].mode()[0]
    df[column].fillna(mode_value, inplace=True)

df[num_cols].isnull().sum()

df[cat_cols].isnull().sum()

for col in cat_cols:
    print(f"{col} has {df[col].nunique()} categories\n")

df

for col in cat_cols:
    print(f"{col} has {df[col].nunique()} categories\n")

unique_categories = df['red_blood_cells'].unique()
print(unique_categories)

rows_with_good_peda_edema = df[df['peda_edema'] == 'good']
rows_with_good_peda_edema

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for col in cat_cols:
  df[col] = le.fit_transform(df[col])

le = LabelEncoder()
df['class'] = le.fit_transform(df['class'])

df.head(100)

feature_columns = [col for col in df.columns if col != 'class']
target_column = 'class'
X = df.drop(columns=['class'])
y = df['class']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)

pip install scikit-optimize

from skopt import BayesSearchCV

from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features (optional but recommended)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the hyperparameter search space
logistic_hyperparameters = {
    'C': Real(1e-6, 1e+6, prior='log-uniform'),
    'penalty': Categorical(['l1', 'l2', 'elasticnet']),
    'solver': Categorical(['saga']),
    'l1_ratio': Real(0, 1),  # Define a range for l1_ratio
}

svm_hyperparameters = {
    'C': Real(1e-6, 1e+6, prior='log-uniform'),
    'kernel': Categorical(['linear', 'rbf', 'poly']),
    'degree': Integer(1, 5),
}

decision_tree_hyperparameters = {
    'criterion': Categorical(['gini', 'entropy']),
    'max_depth': Integer(1, 20),
    'min_samples_split': Integer(2, 20),
}

# Create Bayesian optimization models for each classifier
logistic_opt = BayesSearchCV(
    estimator=LogisticRegression(),
    search_spaces=logistic_hyperparameters,
    n_iter=50,
    random_state=42,
    cv=5,
    n_jobs=-1,
)

svm_opt = BayesSearchCV(
    estimator=SVC(),
    search_spaces=svm_hyperparameters,
    n_iter=50,
    random_state=42,
    cv=5,
    n_jobs=-1,
)

decision_tree_opt = BayesSearchCV(
    estimator=DecisionTreeClassifier(),
    search_spaces=decision_tree_hyperparameters,
    n_iter=50,
    random_state=42,
    cv=5,
    n_jobs=-1,
)

# Fit the Bayesian optimization models
logistic_opt.fit(X_train, y_train)
svm_opt.fit(X_train, y_train)
decision_tree_opt.fit(X_train, y_train)

# Get the best hyperparameters for each classifier
best_logistic_params = logistic_opt.best_params_
best_svm_params = svm_opt.best_params_
best_decision_tree_params = decision_tree_opt.best_params_

# Train models with the best hyperparameters
best_logistic = LogisticRegression(**best_logistic_params)
best_svm = SVC(**best_svm_params)
best_decision_tree = DecisionTreeClassifier(**best_decision_tree_params)

best_logistic.fit(X_train, y_train)
best_svm.fit(X_train, y_train)
best_decision_tree.fit(X_train, y_train)

# Evaluate the models on the test data
y_pred_logistic = best_logistic.predict(X_test)
y_pred_svm = best_svm.predict(X_test)
y_pred_decision_tree = best_decision_tree.predict(X_test)

accuracy_logistic = accuracy_score(y_test, y_pred_logistic)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
accuracy_decision_tree = accuracy_score(y_test, y_pred_decision_tree)

precision_logistic = precision_score(y_test, y_pred_logistic)
precision_svm = precision_score(y_test, y_pred_svm)
precision_decision_tree = precision_score(y_test, y_pred_decision_tree)

recall_logistic = recall_score(y_test, y_pred_logistic)
recall_svm = recall_score(y_test, y_pred_svm)
recall_decision_tree = recall_score(y_test, y_pred_decision_tree)

f1_logistic = f1_score(y_test, y_pred_logistic)
f1_svm = f1_score(y_test, y_pred_svm)
f1_decision_tree = f1_score(y_test, y_pred_decision_tree)

print('Logistic Regression - Best Hyperparameters:', best_logistic_params)
print(f'Logistic Regression Accuracy: {accuracy_logistic:.2f}')
print(f'Logistic Regression Precision: {precision_logistic:.2f}')
print(f'Logistic Regression Recall: {recall_logistic:.2f}')
print(f'Logistic Regression F1 Score: {f1_logistic:.2f}')

print('SVM - Best Hyperparameters:', best_svm_params)
print(f'SVM Accuracy: {accuracy_svm:.2f}')
print(f'SVM Precision: {precision_svm:.2f}')
print(f'SVM Recall: {recall_svm:.2f}')
print(f'SVM F1 Score: {f1_svm:.2f}')

print('Decision Tree - Best Hyperparameters:', best_decision_tree_params)
print(f'Decision Tree Accuracy: {accuracy_decision_tree:.2f}')
print(f'Decision Tree Precision: {precision_decision_tree:.2f}')
print(f'Decision Tree Recall: {recall_decision_tree:.2f}')
print(f'Decision Tree F1 Score: {f1_decision_tree:.2f}')